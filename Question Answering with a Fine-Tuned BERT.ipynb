{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe753e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (4.15.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: requests in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0edce74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tjdgu\\anaconda3\\lib\\site-packages (from torch) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee40a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8dc5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many parameters does BERT-large have?\"\n",
    "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79148278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
    "# tokenizer's behavior, let's also get the token strings and display them.\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "# For each token and its id...\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    \n",
    "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    \n",
    "    # Print the token string and its ID in two columns.\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the input_ids for the first instance of the `[SEP]` token.\n",
    "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "# The number of segment A tokens includes the [SEP] token istelf.\n",
    "num_seg_a = sep_index + 1\n",
    "\n",
    "# The remainder are segment B.\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "# Construct the list of 0s and 1s.\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "# There should be a segment_id for every input token.\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our example through the model.\n",
    "outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                             token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
    "                             return_dict=True) \n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fcd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the tokens with the highest `start` and `end` scores.\n",
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "\n",
    "# Combine the tokens in the answer and print it out.\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f18385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the first token.\n",
    "answer = tokens[answer_start]\n",
    "\n",
    "# Select the remaining answer tokens and join them with whitespace.\n",
    "for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "    # If it's a subword token, then recombine it with the previous token.\n",
    "    if tokens[i][0:2] == '##':\n",
    "        answer += tokens[i][2:]\n",
    "    \n",
    "    # Otherwise, add a space then the token.\n",
    "    else:\n",
    "        answer += ' ' + tokens[i]\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "#sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the scores out of PyTorch Tensors and convert them to 1D numpy arrays.\n",
    "s_scores = start_scores.detach().numpy().flatten()\n",
    "e_scores = end_scores.detach().numpy().flatten()\n",
    "\n",
    "# We'll use the tokens as the x-axis labels. In order to do that, they all need\n",
    "# to be unique, so we'll add the token index to the end of each one.\n",
    "token_labels = []\n",
    "for (i, token) in enumerate(tokens):\n",
    "    token_labels.append('{:} - {:>2}'.format(token, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example through the model.\n",
    "    outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
    "                    return_dict=True) \n",
    "\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dbb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n",
    "\n",
    "print(wrapper.fill(bert_abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What does the 'B' in BERT stand for?\"\n",
    "\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are some example applications of BERT?\"\n",
    "\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a0cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is your name?\"\n",
    "answer_question(question,, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is your name?\"\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is BERT?\"\n",
    "answer_question(question,z, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507780f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is BERT?\"\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62aa4e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 248 tokens.\n",
      "\n",
      "Answer: \"bid\"\n"
     ]
    }
   ],
   "source": [
    "question = \"BERT\"\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484f2de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전적인 의미로의 뇌진탕은 구조변화 없이 기능적인 소실만이 잠시 일시적으로 나타났다 회복되는 것이므로 외상과 연관된 후유증은 없는 가벼운 상태로\n",
      "규정되어 왔습니다. 그러나 엄밀한 의미로 구조적인 손상이 전혀 없다고 하기는 어렵다는 것이 최근의 견해입니다. 의식 소실의 시간이나 기억상실의\n",
      "시간이 길다면 영구적인 기능 감소가 발생할 소지가 있는 것입니다. 가장 흔히 관찰되는 후유증은 뇌진탕 후 증후군(Post-Concussion\n",
      "syndrome)이라고 하며 주 증상으로는 두통 및 뇌신경관련 증상으로 어지럼증, 귀울림(이명), 청력감퇴, 시력장애 등을 호소합니다. 정신과적\n",
      "증상으로 과민, 불안, 우울, 인격변화, 피로, 수면장애, 인지장애, 기억장애, 집중력 및 주의력 장애 등을 호소합니다. 대부분의 경우 3개월\n",
      "내에 증상이 없어지지만, 소수의 경우 1년 이상 지속될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "bert_abstract = \"사전적인 의미로의 뇌진탕은 구조변화 없이 기능적인 소실만이 잠시 일시적으로 나타났다 회복되는 것이므로 외상과 연관된 후유증은 없는 가벼운 상태로 규정되어 왔습니다. 그러나 엄밀한 의미로 구조적인 손상이 전혀 없다고 하기는 어렵다는 것이 최근의 견해입니다. 의식 소실의 시간이나 기억상실의 시간이 길다면 영구적인 기능 감소가 발생할 소지가 있는 것입니다. 가장 흔히 관찰되는 후유증은 뇌진탕 후 증후군(Post-Concussion syndrome)이라고 하며 주 증상으로는 두통 및 뇌신경관련 증상으로 어지럼증, 귀울림(이명), 청력감퇴, 시력장애 등을 호소합니다. 정신과적 증상으로 과민, 불안, 우울, 인격변화, 피로, 수면장애, 인지장애, 기억장애, 집중력 및 주의력 장애 등을 호소합니다. 대부분의 경우 3개월 내에 증상이 없어지지만, 소수의 경우 1년 이상 지속될 수 있습니다.\"\n",
    "\n",
    "print(wrapper.fill(bert_abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20662ae4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer_question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_23912/3547565724.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mquestion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"뇌진탕의 후유증의 지속 기간은?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0manswer_question\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbert_abstract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'answer_question' is not defined"
     ]
    }
   ],
   "source": [
    "question = \"뇌진탕의 후유증의 지속 기간은?\"\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7038bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "농가진 또는 농피증은 소아에서 가장 흔한 피부 감염증입니다. 황색 포도알균 또는 A군 사슬알균 등의 세균이 원인입니다. 주로 피부에 상처가\n",
      "생기거나 벌레에 물린 후에 발생합니다. 작은 붉은 반점으로 시작해 얇은 수포나 농포가 생긴 후 터집니다. 터진 수포에서 노란 진물이 나오고\n",
      "딱지가 앉으며, 때로 가려움이 동반되지만 통증이나 전신적인 발열은 거의 없습니다. 손이나 옷을 통해 피부의 다른 부위로 번질 수 있습니다. 피부\n",
      "병변 부위를 깨끗이 씻어내고 항생제 연고를 국소적으로 발라줍니다.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "bert_abstract = \"농가진 또는 농피증은 소아에서 가장 흔한 피부 감염증입니다. 황색 포도알균 또는 A군 사슬알균 등의 세균이 원인입니다. 주로 피부에 상처가 생기거나 벌레에 물린 후에 발생합니다. 작은 붉은 반점으로 시작해 얇은 수포나 농포가 생긴 후 터집니다. 터진 수포에서 노란 진물이 나오고 딱지가 앉으며, 때로 가려움이 동반되지만 통증이나 전신적인 발열은 거의 없습니다. 손이나 옷을 통해 피부의 다른 부위로 번질 수 있습니다. 피부 병변 부위를 깨끗이 씻어내고 항생제 연고를 국소적으로 발라줍니다.\"\n",
    "\n",
    "print(wrapper.fill(bert_abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3478f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 454 tokens.\n",
      "\n",
      "Answer: \"[CLS]\"\n"
     ]
    }
   ],
   "source": [
    "question = \"소아에서 가장 흔하게 발생하는 피부 감염증은?\"\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c61a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-6.4849, -6.4358, -8.1077, -8.8489, -7.8751, -8.0522, -8.4684, -8.5295,\n",
      "         -7.7074, -9.2464, -6.4849, -2.7303, -6.3473, -5.7299, -7.7780, -7.0391,\n",
      "         -6.3331, -7.3153, -7.3048, -7.4121, -2.2534, -5.3971, -0.9424, -7.3584,\n",
      "         -5.4575, -7.0769, -4.4887, -3.9272, -5.6967, -5.9506, -5.0059, -5.9812,\n",
      "          0.0530, -5.5968, -4.7093, -4.5750, -6.1786, -2.2294, -0.1904, -0.2327,\n",
      "         -2.7331,  6.4256, -2.6543, -4.5655, -4.9872, -4.9834, -5.9110, -7.8402,\n",
      "         -1.8986, -7.2123, -4.1543, -6.2354, -8.0953, -7.2329, -6.4411, -6.8384,\n",
      "         -8.1032, -7.0570, -7.7332, -6.8711, -7.1045, -8.2966, -6.1939, -8.0817,\n",
      "         -7.5501, -5.9695, -8.1007, -6.8849, -8.2273, -6.4850]],\n",
      "       grad_fn=<CloneBackward0>), end_logits=tensor([[-2.0629, -6.3878, -6.2450, -6.3605, -7.0722, -7.6281, -7.1160, -6.8674,\n",
      "         -7.1313, -7.1495, -2.0628, -5.0858, -4.7276, -3.5955, -6.3050, -7.1109,\n",
      "         -4.4975, -4.7221, -5.4760, -5.5441, -6.1391, -5.8593, -0.4636, -4.3720,\n",
      "         -1.0411, -5.3359, -6.2969, -6.1156, -5.1736, -4.6144, -4.8274, -6.3638,\n",
      "         -4.2078, -5.2329, -4.7127,  0.7952, -0.7376, -4.5555, -5.2985, -3.6082,\n",
      "         -3.7726,  2.7501,  5.4644,  4.1220,  1.2127, -5.5042, -5.8367, -6.0745,\n",
      "         -3.8426, -5.8273, -1.9782, -1.3083, -2.4872, -5.3204, -6.5550, -6.3885,\n",
      "         -6.8736, -6.3949, -7.0454, -6.0590, -4.5225, -6.6686, -4.0074, -6.9146,\n",
      "         -6.9742, -6.5173, -4.8760, -4.4629, -4.7580, -2.0631]],\n",
      "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7db117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f4065b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be92a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.4849, -6.4358, -8.1077, -8.8489, -7.8751, -8.0522, -8.4684, -8.5295,\n",
      "         -7.7074, -9.2464, -6.4849, -2.7303, -6.3473, -5.7299, -7.7780, -7.0391,\n",
      "         -6.3331, -7.3153, -7.3048, -7.4121, -2.2534, -5.3971, -0.9424, -7.3584,\n",
      "         -5.4575, -7.0769, -4.4887, -3.9272, -5.6967, -5.9506, -5.0059, -5.9812,\n",
      "          0.0530, -5.5968, -4.7093, -4.5750, -6.1786, -2.2294, -0.1904, -0.2327,\n",
      "         -2.7331,  6.4256, -2.6543, -4.5655, -4.9872, -4.9834, -5.9110, -7.8402,\n",
      "         -1.8986, -7.2123, -4.1543, -6.2354, -8.0953, -7.2329, -6.4411, -6.8384,\n",
      "         -8.1032, -7.0570, -7.7332, -6.8711, -7.1045, -8.2966, -6.1939, -8.0817,\n",
      "         -7.5501, -5.9695, -8.1007, -6.8849, -8.2273, -6.4850]],\n",
      "       grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.start_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cabc2ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0629, -6.3878, -6.2450, -6.3605, -7.0722, -7.6281, -7.1160, -6.8674,\n",
      "         -7.1313, -7.1495, -2.0628, -5.0858, -4.7276, -3.5955, -6.3050, -7.1109,\n",
      "         -4.4975, -4.7221, -5.4760, -5.5441, -6.1391, -5.8593, -0.4636, -4.3720,\n",
      "         -1.0411, -5.3359, -6.2969, -6.1156, -5.1736, -4.6144, -4.8274, -6.3638,\n",
      "         -4.2078, -5.2329, -4.7127,  0.7952, -0.7376, -4.5555, -5.2985, -3.6082,\n",
      "         -3.7726,  2.7501,  5.4644,  4.1220,  1.2127, -5.5042, -5.8367, -6.0745,\n",
      "         -3.8426, -5.8273, -1.9782, -1.3083, -2.4872, -5.3204, -6.5550, -6.3885,\n",
      "         -6.8736, -6.3949, -7.0454, -6.0590, -4.5225, -6.6686, -4.0074, -6.9146,\n",
      "         -6.9742, -6.5173, -4.8760, -4.4629, -4.7580, -2.0631]],\n",
      "       grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbc3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
